This paper proposed the Unison Cache, a large die-stacked DRAM cache structure. The authors claim that Unison Cache have combined the advantages of two state-of-the-art cache designs, i.e. Alloy Cache which is block-based, and Footprint Cache which is page-based. Experiments have shown that Unison cache have achieved high cache hit rate, together with scalability, when compared to current designs.

Recent advances in die-stacking technologies have enabled the integration of DRAM in the same package as the processor. However, current size of die-stacked DRAM cannot provide enough memory capacity for current machines, especially servers, so most researches on die-stacked DRAMs see it as a huge last-level cache. Such design has encountered several problems, for example, there would be a large amount of tag overhead if conventional block size is used. In addition, most temporal locality has been intercepted at L1 and L2 level, so we will see a high miss rate for die-stacked DRAMs. Current proposals for DRAM structure can’t deal with all these problems simultaneously. For example, Alloy Cache (AC) is a block-based DRAM cache. Due to the large amount of its page numbers, the tags will have to be placed in DRAM instead of DRAM. AC mitigates its tag lookup latency by collocating tags with each data block. However, AC have a relatively low hit rate, because blocks are fetched at a small granularity. Another proposal is the Footprint Cache (FC), a page-based cache. FC have higher hit rates, since fetching large pages instead of blocks exploits spatial locality, which is still available at lower levels of the cache hierarchy. FC prevents wasting off-chip memory bandwidth by fetching only data that are going to be touched during a page’s residency in the DRAM cache with a predictor. The footprint is associated with a (PC, offset) pair and stored in a SRAM history table. Page-based designs allows us to put the tags in on-chip SRAMs, which results in lower tag storage latency. However, such FC design is not scalable, since tag storage size will quickly reach several MBs, which cannot be built in to on-chip SRAMs. In this paper, the Unison Cache is proposed, which tries to combine the advantages of AC and FC. Unison cache uses a single tag per page, and each tag maintains its blocks using valid and dirty bit vectors. Each page comes with a (PC, offset) pair, which identifies the first access instruction and offset to the page. When the page resides in the cache, every access to the page will update the corresponding valid/dirty bit, which will in turn update the footprint prediction table when the page is evicted when there are underprediction of overprediction. Based on the observation that many pages only have one of their blocks accessed before being evicted, Unison Cache employs singleton prediction method as an optimization. For singleton pages, Unison Cache does not allocate a page in cache. Instead, the accessed block is simply forwarded to the requester. Experiment results show that Unison cache generally have better performance over AC and FC. It is also observed that using 960B pages yield better results that a 1984B page size. 

In conclusion, the Unison Cache structure proposed in this paper combines the advantages of both block and page based designs, and achieves high hit rate and low latency simultaneously. However, most assumptions and experiments in this paper are based on server workload. It is not clear that if such design would still be optimal under other workloads, which may have different memory access patterns. 
