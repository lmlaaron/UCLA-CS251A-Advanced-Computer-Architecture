This paper proposed two kinds of caches, victim caches (based on miss caches) and stream buffers, in order to mitigate the performance loss caused by cache misses. The authors first identified the problem of the increasing gap between CPU cycle and cache miss latency, proposed new cache architectures, and gave experiment results with analysis.

Back in 1990, people noticed that CPU frequencies are being improved much faster than memory access times, and cache misses began to play a major role in overall system performance. Designing better memory hierarchies to exploit program locality have shown great potential in improving performance. The authors observed in the baseline design that a fair amount of cache misses are conflict misses, because L1 cache are assumed to be direct-mapped. To reduce the number of conflict cache misses, miss caching is proposed, which is a fully-associative cache between L1 and L2. Although miss caching have shown to be effective in reducing conflict misses, especially for L1 instruction cache, the authors noticed that there is duplication of data between L1 cache and miss cache. Victim caching is then proposed, which shares similar architecture with miss caching, but caches only the victims from L1 cache. By analysis we can see that victim caching is always an improvement over miss caching, which is also proved by the experiment results. Miss caching and victim caching have reduced the number of conflict misses, but have shown little effect on other types of cache misses. To address compulsory misses and capacity misses, the authors then proposed stream buffers, which is an improvement over traditional prefetch techniques. For example, in tagged prefetch a block is fetched when its previous block is used. However, tagged prefetch is not fast enough under high memory latencies, as prefetch blocks can’t be received before execution on previous block is finished. Based on this observation, the authors proposed streamed buffer, which is similar to a FIFO queue to fetch as many sequential blocks as possible, instead of fetching only one block ahead. However, although a single stream buffer is effective on instruction caches, its shows limited improvements on data caches. This is partly because data references tend to be interleaved, and stream buffers have to be frequently flushed. To solve this problem, the multi-way stream buffers are proposed, which consists of several stream buffers in parallel to prevent frequent flushing. It is shown in experiments that multi-way stream buffers have a better performance on reducing data cache misses than (one-way) stream buffers. 

This paper makes a nice analysis on impacts of cache misses, and proposed two novel cache architectures to address specific types of cache misses. The experiment results are convincing and promising. However, there are certain assumption that may not hold true today. For example, the author assumed that both L1 and L2 cache are direct-mapped. In contrast, in today’s Intel Core i7 architecture, L1 and L2 caches are usually 4-way or 8-way associative. In addition, the author assumed in the baseline architecture that L2 caches are off-chip, but improvements in manufacturing technology have allowed us to put L2 cache (and even shared L3 cache) on chip. With each level in the memory hierarchy moving closer to the CPU, it is likely that the structures proposed in this paper will be less effective in today’s scenario.
